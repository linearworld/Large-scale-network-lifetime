{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a460e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.font_manager as font_manager\n",
    "import copy\n",
    "from scipy.stats import linregress\n",
    "from matplotlib.ticker import LogLocator, MultipleLocator, MaxNLocator\n",
    "title_font = font_manager.FontProperties(family='Times New Roman', size=16, weight='normal')\n",
    "label_font = font_manager.FontProperties(family='Times New Roman', size=16, weight='bold')\n",
    "legend_font = font_manager.FontProperties(family='Times New Roman', size=14, weight='normal')\n",
    "legend_small_font = font_manager.FontProperties(family='Times New Roman', size=12, weight='normal')\n",
    "\n",
    "size_marker=90\n",
    "special_tick_size=15\n",
    "\n",
    "#output ticks (base10)\n",
    "def tranform_ticks_log(log_value_min,log_value_max,max_to_show=3):\n",
    "    if(log_value_max-log_value_min<=1.3):\n",
    "        return [10**int((log_value_min+log_value_max)/2)]\n",
    "    seperate=list(range(int(log_value_min),int(log_value_max)+1))\n",
    "    if(len(seperate)<=max_to_show):\n",
    "        ss=seperate\n",
    "    else:\n",
    "        add=int((log_value_max-log_value_min)/max_to_show)+1\n",
    "        z=int(log_value_min)\n",
    "        ss=[]\n",
    "        while(len(ss)<max_to_show):\n",
    "            ss.append(z)\n",
    "            z+=add\n",
    "    ticks=[]\n",
    "    for s in ss:\n",
    "        ticks.append(10**s)\n",
    "    return ticks\n",
    "\n",
    "#output ticks (base2)\n",
    "def tranform_ticks_log2(log_value_min,log_value_max,max_to_show=3):\n",
    "    seperate=list(range(int(log_value_min),int(log_value_max)+1))\n",
    "    if(len(seperate)<=max_to_show):\n",
    "        ss=seperate\n",
    "    else:\n",
    "        add=int((log_value_max-log_value_min)/max_to_show)+1\n",
    "        z=int(log_value_min)\n",
    "        ss=[]\n",
    "        while(len(ss)<max_to_show):\n",
    "            ss.append(z)\n",
    "            z+=add\n",
    "    ticks=[]\n",
    "    for s in ss:\n",
    "        ticks.append(10**s)\n",
    "    return ticks\n",
    "\n",
    "\n",
    "\n",
    "def partitionGroundTruthSamples(groundTruthSamples,seperation=20):\n",
    "    \"\"\"\n",
    "    This function divide empirical distribution into seperation=20 intervals,\n",
    "    each interval has equal samples.\n",
    "    \"\"\"\n",
    "    \n",
    "    interpoints=[] #\n",
    "    groundTruthSamples.sort()\n",
    "    for _ in range(1,seperation):\n",
    "        rank=int((_/seperation)*len(groundTruthSamples))\n",
    "        interpoints.append(groundTruthSamples[rank])\n",
    "    return interpoints\n",
    "\n",
    "def KL_div_calGivenInterpoints(interpoints,samples,groundTruthSamples):\n",
    "    \"\"\"\n",
    "    this function calculate KL-divergence between samples and ground truth given interpoints\n",
    "    \"\"\"\n",
    "    P=[] # Discrite distribution for samples in given interpoints\n",
    "    Q=[] # Discrite distribution for samples in groundTruthSamples\n",
    "    interpoints=list(interpoints)\n",
    "    interpoints=[-float('inf')]+interpoints+[float('inf')]\n",
    "    for index in range(len(interpoints)-1):\n",
    "        up_interval=interpoints[index+1]\n",
    "        down_interval=interpoints[index]\n",
    "        count = len([x for x in samples if down_interval <= x < up_interval])\n",
    "        P.append(count/len(samples))\n",
    "        count = len([x for x in groundTruthSamples if down_interval <= x < up_interval])\n",
    "        Q.append(count/len(groundTruthSamples))\n",
    "    return basic_module_for_KL(P,Q)\n",
    "def basic_module_for_KL(P,Q):\n",
    "    \"\"\"\n",
    "    Input：two discrite probability distribution \n",
    "    Return：KL_divergence\"\"\"\n",
    "    assert(len(P)==len(Q))\n",
    "    kl=0\n",
    "    for index in range(len(P)):\n",
    "        if(P[index]!=0):\n",
    "            kl+=P[index]*np.log(P[index]/Q[index])\n",
    "    return kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88359eff",
   "metadata": {},
   "source": [
    "# Scaling relationships fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_list=100*2**np.array(range(2,10))\n",
    "#N_list=[400,565,800,1131,1600,2262,3200,4525,6400,9050,12800,18101,25600,36203,51200]\n",
    "print(N_list[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b216786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing: read node lifetime file and output network lifetime\n",
    "# network lifetime is the time when 20% nodes dead.\n",
    "\n",
    "alpha=0.8 # network lifetime is the time when fraction of living nodes larger than alpha=80%\n",
    "for N in N_list:\n",
    "    file_name_to_write='./net_percentile_lifetime/PercentileLife_N='+str(N)+'.txt'\n",
    "    f=open(file_name_to_write,'w')\n",
    "    f.close()\n",
    "    id_count=0\n",
    "    file_name_to_read='./node_lifetime_log/N='+str(N)+'/nodeLife_N='+str(N)+\"_id=\"+str(id_count)+'.txt'\n",
    "    while(os.path.exists(file_name_to_read)):\n",
    "        node_lifetime=[]\n",
    "        f=open(file_name_to_read,'r')\n",
    "        line=f.readline()\n",
    "        while(line!=''):\n",
    "            node_lifetime.append(int(line))\n",
    "            line=f.readline()\n",
    "        f.close()\n",
    "        node_lifetime.sort()\n",
    "        net_life=node_lifetime[int(alpha*N)]\n",
    "        file_name_to_write='./net_percentile_lifetime/PercentileLife_N='+str(N)+'.txt'\n",
    "        f=open(file_name_to_write,'a')\n",
    "        f.write(str(net_life)+\"\\n\")\n",
    "        f.close()    \n",
    "        id_count+=1\n",
    "        file_name_to_read='./node_lifetime_log/N='+str(N)+'/nodeLife_N='+str(N)+\"_id=\"+str(id_count)+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a49d11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Read NetLife and demonstrate their distribution, mean value and standard deviation\n",
    "N_list=100*2**np.array(range(2,10))\n",
    "#N_list=[400,565,800,1131,1600,2262,3200,4525,6400,9050,12800,18101,25600,36203,51200]\n",
    "\n",
    "std_all_list=[]\n",
    "mean_all_list=[]\n",
    "for N in N_list:\n",
    "    net_lifetime_list=[]\n",
    "    file_PercentileNetLife='./net_percentile_lifetime/PercentileLife_N='+str(N)+'.txt'\n",
    "    f=open(file_PercentileNetLife,'r')\n",
    "    line=f.readline()\n",
    "    while(line!=''):\n",
    "        net_lifetime_list.append(int(line))\n",
    "        line=f.readline()\n",
    "    f.close()\n",
    "    net_lifetime_list=net_lifetime_list[:150]\n",
    "    plt.hist(net_lifetime_list,bins=20)\n",
    "    std_all_list.append(np.std(net_lifetime_list))\n",
    "    mean_all_list.append(np.mean(net_lifetime_list))\n",
    "    plt.title(\"N=\"+str(N))\n",
    "    plt.xlabel(\"Lifetime\",)\n",
    "    plt.ylabel(\"Distribution\",)\n",
    "    plt.show()\n",
    "\n",
    "plt.scatter(N_list,std_all_list)\n",
    "#plt.plot(N_list,500*np.array(N_list)**(-0.4))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('Standard Variation')\n",
    "plt.xlabel(\"System size N\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(N_list,mean_all_list)\n",
    "#plt.plot(N_list,500*np.array(N_list)**(-0.4))\n",
    "plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "plt.ylabel('mean value')\n",
    "plt.xlabel(\"System size N\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ffd1e",
   "metadata": {},
   "source": [
    "# Scaling law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c8192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a4e12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "line_width_marker_set=2\n",
    "dpi_set=1000\n",
    "max_tick_num=4\n",
    "num_train=5\n",
    "L_list_train=N_list[:num_train]\n",
    "L_list_predict=N_list[num_train:]\n",
    "mean_list=mean_all_list[:num_train]\n",
    "std_list=std_all_list[:num_train]\n",
    "\n",
    "mean_predict_list=mean_all_list[num_train:]\n",
    "std_predict_list=std_all_list[num_train:]\n",
    "L_show_list=np.array(list(L_list_train)+list(L_list_predict))\n",
    "\n",
    "\n",
    "x_to_train=np.array(L_list_train,dtype='float')\n",
    "y_to_train=mean_list\n",
    "x_to_predict=np.array(L_list_predict,dtype='float')\n",
    "y_to_predict=np.array(mean_predict_list)\n",
    "\n",
    "delta=1 #——————————————————————————————参数： 收敛速率delta\n",
    "power_for_weight_of_size=2  #——————————————————误差权重中 规模的幂次\n",
    "renormalized_x_to_train=x_to_train/np.sum(x_to_train)\n",
    "\n",
    "def target_function(x,y0,c1):\n",
    "    return y0+c1*x**(-delta)\n",
    "def loss_SQ(parms):\n",
    "    y0=parms[0]\n",
    "    c1=parms[1]\n",
    "    y_predict_list=y0+c1*x_to_train**(-delta)\n",
    "    res=y_predict_list-y_to_train\n",
    "    res=res*renormalized_x_to_train**power_for_weight_of_size/np.sum(renormalized_x_to_train**power_for_weight_of_size)\n",
    "    return np.sum(res**2)\n",
    "\n",
    "initial_guess=[0,1]\n",
    "result = minimize(loss_SQ, initial_guess, method='BFGS')\n",
    "\n",
    "params=result.x\n",
    "best_mean_limit,c1=params #parameters fitted from small-scale network lifetime data\n",
    "\n",
    "plt.scatter(L_list_train,np.abs(best_mean_limit-mean_list),label='Known',edgecolors='#1f77b4', facecolors='none',marker='o',linewidth=line_width_marker_set,s=size_marker)\n",
    "plt.scatter(L_list_predict,np.abs(best_mean_limit-mean_predict_list),label='Unknown',edgecolors='#1f77b4', facecolors='none',marker='^',linewidth=line_width_marker_set,s=size_marker)\n",
    "x_to_show=np.linspace(np.min(x_to_train),np.max(x_to_predict),1000)\n",
    "y_predict=target_function(x_to_show,best_mean_limit,c1)\n",
    "plt.plot(x_to_show, np.abs(y_predict-best_mean_limit), color='red', label='Fit',linestyle='--')\n",
    "r_value=np.corrcoef(x_to_show,np.abs(y_predict-best_mean_limit))[0][1]\n",
    "print(\"R2=\"+str(round(r_value**2,3)))\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "#ticks=tranform_ticks_log(np.min(value_list),np.max(value_list),max_tick_num)\n",
    "#ticks=[0.2,0.3,0.4,0.6]\n",
    "#plt.yticks(ticks)  \n",
    "\n",
    "plt.xlabel(\"System Size \"+r\"$N$\",fontproperties=label_font)\n",
    "plt.ylabel(r\"$<t_{\\infty}>-<t_N>$\",fontproperties=label_font)\n",
    "#plt.title(net_name_show+\" (\"+\"power=\"+str(power)+\")\",fontproperties=label_font)\n",
    "plt.legend(prop=legend_font)\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "filename='Mean_loglog.png'\n",
    "plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "x_min_to_show=((np.min(mean_list)-best_mean_limit)/c1)**(1/(-delta))\n",
    "\n",
    "\n",
    "x_show_list=np.linspace(x_min_to_show,L_list_predict[-1],100)\n",
    "\n",
    "plt.scatter(L_list_train,mean_list,label='Known',edgecolors='#1f77b4', facecolors='none',marker='o',linewidth=line_width_marker_set,s=size_marker)\n",
    "plt.scatter(L_list_predict,mean_predict_list,label='Unknown',edgecolors='#1f77b4', facecolors='none',marker='^',linewidth=line_width_marker_set,s=size_marker)\n",
    "\n",
    "plt.plot(x_show_list, target_function(x_show_list,best_mean_limit,c1), color='red', label='Fit',linestyle='--')\n",
    "plt.xlabel(\"System Size \"+r\"$N$\",fontproperties=label_font)\n",
    "plt.ylabel(r\"$<t_N>$\",fontproperties=label_font)\n",
    "#plt.title(net_name_show+\" (\"+\"power=\"+str(power)+\")\",fontproperties=label_font)\n",
    "plt.legend(prop=legend_font)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "filename='Mean_predict.png'\n",
    "plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(L_list_train,std_list,label='Known',edgecolors='#1f77b4', facecolors='none',marker='o',linewidth=line_width_marker_set,s=size_marker)\n",
    "plt.scatter(L_list_predict,std_predict_list,label='Unknown',edgecolors='#1f77b4', facecolors='none',marker='^',linewidth=line_width_marker_set,s=size_marker)\n",
    "\n",
    "x_list=np.log(L_list_train)\n",
    "y_list=np.log(std_list)\n",
    "slope=-1/2\n",
    "intercept=np.mean(y_list)-slope*np.mean(x_list)\n",
    "\n",
    "x_show_list=np.log(np.array(L_show_list))\n",
    "y_show_list=slope * x_show_list + intercept\n",
    "\n",
    "alpha=copy.deepcopy(slope) #——————————————————————————————————第三个参数： 缩放参数alpha\n",
    "regression_line = slope * x_list + intercept\n",
    "#plt.plot(np.exp(x_list), np.exp(regression_line), color='red', label=str(round(np.exp(intercept),4))+r'$N^{{{}}}$'.format(round(slope,4)))\n",
    "plt.plot(np.exp(x_show_list), np.exp(y_show_list), color='red', label=\"Fit\",linestyle='--')\n",
    "print(str(round(np.exp(intercept),4))+r'$N^{{{}}}$'.format(round(slope,4)))\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "#value_list=np.abs(best_mean_limit-mean_list)\n",
    "#ticks=tranform_ticks_log(np.min(value_list),np.max(value_list),max_tick_num)\n",
    "#plt.yticks(yticks)  \n",
    "\n",
    "plt.xlabel(\"System Size \"+r\"$N$\",fontproperties=label_font)\n",
    "plt.ylabel(r\"Standard deviation $\\chi_t(N)$\",fontproperties=label_font)\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "#plt.title(net_name_show+\" (\"+\"power=\"+str(power)+\")\",fontproperties=label_font)\n",
    "plt.legend(prop=legend_font)\n",
    "filename='std_predict.png'\n",
    "plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "marker_list=['^','v','>','<','o','d','*','1','2','3','4','+','x','|','_']\n",
    "count_L=0\n",
    "for L in np.array(list(L_list_train)):\n",
    "    marker=marker_list[count_L]\n",
    "    count_L+=1\n",
    "    N=L\n",
    "    lifetime_data=[]\n",
    "    file_name='./net_percentile_lifetime/PercentileLife_N='+str(L)+'.txt'\n",
    "    f=open(file_name,'r')\n",
    "    line=f.readline()\n",
    "    while(line!=''):\n",
    "        lifetime_data.append(float(line))\n",
    "        line=f.readline()\n",
    "    f.close()\n",
    "    lifetime_data=lifetime_data[:150]\n",
    "    min_life=np.min(lifetime_data)\n",
    "    max_life=np.max(lifetime_data)\n",
    "    cut_list=np.linspace(min_life,max_life,20)\n",
    "    mid_list=[]\n",
    "    distri_value_list=[]\n",
    "    for index_cut in range(len(cut_list)-1):\n",
    "        down=cut_list[index_cut]\n",
    "        up=cut_list[index_cut+1]\n",
    "        mid_list.append(((up+down)/2))\n",
    "        distri_value_list.append(np.sum((lifetime_data>down)&(lifetime_data<up))/(len(lifetime_data)*(up-down)))\n",
    "    distri_value_list=np.array(distri_value_list)\n",
    "    mid_list=np.array(mid_list)\n",
    "    plt.scatter((mid_list-np.mean(lifetime_data))/N**alpha,distri_value_list*N**alpha,label='N='+str(N),marker=marker,s=size_marker)\n",
    "    plt.plot((mid_list-np.mean(lifetime_data))/N**alpha,distri_value_list*N**alpha,linewidth=1)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "#plt.xticks([-50,0,50])\n",
    "#plt.xlim(-100,150)\n",
    "plt.legend(loc='upper right',prop=legend_small_font)\n",
    "plt.xlabel(\"Rescaled Deviation \"+r\"$(t_N-<t_N>)$\"+r'$N^{{{}}}$'.format(round(-alpha,4)),fontproperties=label_font)\n",
    "plt.ylabel(\"Rescaled Probability \"+r\"$P(t)/$\"+r'$N^{{{}}}$'.format(round(-alpha,4)),fontproperties=label_font)\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size) \n",
    "filename='universalDistribution.png'\n",
    "plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('c1='+str(round(c1,4))+' delta='+str(round(delta,4))+' alpha='+str(round(-alpha,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e3291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"t_infinity=\"+str(best_mean_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2052f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The rescaled samples belong to one universal distribution\n",
    "\n",
    "rescaled_lifetime_dict=dict()\n",
    "all_scaled_samples=[]\n",
    "for L in L_list_train:\n",
    "    N=L\n",
    "    lifetime_data=[]\n",
    "    file_name='./net_percentile_lifetime/PercentileLife_N='+str(L)+'.txt'\n",
    "    f=open(file_name,'r')\n",
    "    line=f.readline()\n",
    "    while(line!=''):\n",
    "        lifetime_data.append(float(line))\n",
    "        line=f.readline()\n",
    "    f.close()\n",
    "    lifetime_data=lifetime_data[:150]\n",
    "    min_life=np.min(lifetime_data)\n",
    "    max_life=np.max(lifetime_data)\n",
    "    cut_list=np.linspace(min_life,max_life,20)\n",
    "    mid_list=[]\n",
    "    distri_value_list=[]\n",
    "    for index_cut in range(len(cut_list)-1):\n",
    "        down=cut_list[index_cut]\n",
    "        up=cut_list[index_cut+1]\n",
    "        mid_list.append(((up+down)/2))\n",
    "        distri_value_list.append(np.sum((lifetime_data>down)&(lifetime_data<up))/(len(lifetime_data)*(up-down)))\n",
    "    rescaled_lifetime_data=(lifetime_data-np.mean(lifetime_data))/N**(alpha)\n",
    "    rescaled_lifetime_data.sort()\n",
    "    rescaled_lifetime_dict[N]=rescaled_lifetime_data\n",
    "    all_scaled_samples=all_scaled_samples+list(rescaled_lifetime_data)\n",
    "    acc_list=np.array(list(range(1,len(rescaled_lifetime_data)+1)))/len(rescaled_lifetime_data)\n",
    "    plt.plot(rescaled_lifetime_data,acc_list,label='N='+str(N))\n",
    "plt.legend(prop=legend_font)\n",
    "\n",
    "plt.legend(prop=legend_font)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "\n",
    "plt.ylabel(\"Cumulative probability\",fontproperties=label_font)\n",
    "plt.xlabel(\"Rescaled Deviation \"+r\"$(t_N-<t_N>)$\"+r'$N^{{{}}}$'.format(round(-alpha,4)),fontproperties=label_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the quantile of rescaled distribution\n",
    "\n",
    "Reliability_list=np.arange(0.99,0.01,-0.01)\n",
    "\n",
    "kappa_list=[]\n",
    "all_scaled_samples.sort()\n",
    "for Reliability in Reliability_list:\n",
    "    rank=int((1-Reliability)*len(all_scaled_samples))\n",
    "    kappa_list.append(all_scaled_samples[rank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413deefb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#predicting cumulative probability function of large-scale network based on universal function and scaling laws\n",
    "for L_to_predict in np.array(list(L_list_predict)):\n",
    "    N_to_predict=L_to_predict\n",
    "    #predict result\n",
    "    Reliability_list=np.arange(0.99,0.01,-0.01)\n",
    "    all_scaled_samples.sort()\n",
    "    t_frac=target_function(float(N_to_predict),best_mean_limit,c1)+np.array(kappa_list)*N_to_predict**(alpha)\n",
    "    plt.plot(t_frac,Reliability_list,label='predict',c='r',linestyle='--')\n",
    "    \n",
    "    lifetime_data=[]\n",
    "    file_name='./net_percentile_lifetime/PercentileLife_N='+str(L_to_predict)+'.txt'\n",
    "    f=open(file_name,'r')\n",
    "    line=f.readline()\n",
    "    while(line!=''):\n",
    "        lifetime_data.append(float(line))\n",
    "        line=f.readline()\n",
    "    f.close()\n",
    "    lifetime_data=lifetime_data[:300]\n",
    "    lifetime_data.sort()\n",
    "    \n",
    "    #1. draw reliablity function (scatter: empirical; line: prediction)\n",
    "    Reliability_discret_list=np.arange(0.99,0.01,-0.02)\n",
    "    t_frac_list=[]\n",
    "    for Reliability in Reliability_discret_list:\n",
    "        rank=int((1-Reliability)*len(lifetime_data))\n",
    "        t_frac_list.append(lifetime_data[rank])\n",
    "    t_show_list=list(t_frac_list)\n",
    "    Reliability_show_list=list(Reliability_discret_list)\n",
    "    plt.scatter(t_frac_list,Reliability_show_list,label='Simulation',s=40,marker='o',edgecolors='#1f77b4', facecolors='none',linewidth=line_width_marker_set)\n",
    "    plt.title(\"N=\"+str(N_to_predict),fontproperties=label_font)\n",
    "    plt.legend(prop=legend_font)\n",
    "    plt.ylabel(\"Reliability\",fontproperties=label_font)\n",
    "    plt.xlabel(\"Time / days\",fontproperties=label_font)\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    filename='predictedReliability_N='+str(N_to_predict)+'.png'\n",
    "    #plt.savefig(\"./fig_case1/\"+filename,dpi=2000,bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    #2. draw cumulative function (scatter: empirical; line: prediction)\n",
    "    cumulative_discret_list=np.arange(0.01,0.99,0.05)\n",
    "    t_frac_list=[]\n",
    "    for cumulative in cumulative_discret_list:\n",
    "        rank=int(cumulative*len(lifetime_data))\n",
    "        t_frac_list.append(lifetime_data[rank])\n",
    "    t_show_list=list(t_frac_list)\n",
    "    cumulative_show_list=list(cumulative_discret_list)\n",
    "    plt.scatter(t_frac_list,cumulative_show_list,label='Simulation',marker='o',edgecolors='#1f77b4', facecolors='none',linewidth=line_width_marker_set,s=size_marker)\n",
    "    plt.plot(t_frac,1-Reliability_list,label='Predict',c='r',linestyle='--')\n",
    "    plt.title(\"N=\"+str(N_to_predict),fontproperties=label_font)\n",
    "    plt.legend(prop=legend_font)\n",
    "    plt.ylabel(\"Cumulative Probability\",fontproperties=label_font)\n",
    "    plt.xlabel(\"Time / days\",fontproperties=label_font)\n",
    "    ax = plt.gca()\n",
    "    #ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    #plt.xticks([15,16,17])\n",
    "    #ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "    #ax.xaxis.set_major_locator(MultipleLocator(1))\n",
    "    plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "    plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "    filename='predictedCumulative_N='+str(N_to_predict)+'.png'\n",
    "    plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    predict_distributions=target_function(float(N_to_predict),best_mean_limit,c1)+np.array(all_scaled_samples)*N_to_predict**(alpha)\n",
    "    lifetime_data=[]\n",
    "    file_name='./net_percentile_lifetime/PercentileLife_N='+str(N_to_predict)+'.txt'\n",
    "    f=open(file_name,'r')\n",
    "    line=f.readline()\n",
    "    while(line!=''):\n",
    "        lifetime_data.append(float(line))\n",
    "        line=f.readline()\n",
    "    f.close()\n",
    "    lifetime_data=lifetime_data[:500]\n",
    "    lifetime_data.sort()\n",
    "    plt.hist(lifetime_data,alpha=0.5,density=True,bins=20,label='GroundTruth')\n",
    "    plt.hist(predict_distributions,alpha=0.5,density=True,bins=20,label='Predict')\n",
    "    plt.legend()\n",
    "    plt.title(\"N=\"+str(N_to_predict),fontproperties=label_font)\n",
    "    plt.ylabel(\"Probability Density\",fontproperties=label_font)\n",
    "    plt.xlabel(\"Time / days\",fontproperties=label_font)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    groundTruthSamples=lifetime_data\n",
    "    if(N_to_predict!=999900):\n",
    "        interpoints=partitionGroundTruthSamples(groundTruthSamples,seperation=10)\n",
    "    else:\n",
    "        interpoints=partitionGroundTruthSamples(groundTruthSamples,seperation=5)\n",
    "    KL_predict=KL_div_calGivenInterpoints(interpoints,predict_distributions,groundTruthSamples)\n",
    "    print(\"KL divergence for N=\"+str(N_to_predict)+\" is \"+str(KL_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting percentile lifetime of large-scale network\n",
    "color_list=['#1f77b4','orange','green','c']\n",
    "marker_list_known=['o','s','D']\n",
    "marker_list_unknown=['^','v','>']\n",
    "\n",
    "Reliability_test_list=[0.01,0.5,0.99]\n",
    "for index_reliability in range(len(Reliability_test_list)):\n",
    "    Reliability=Reliability_test_list[index_reliability]\n",
    "    color=color_list[index_reliability]\n",
    "    marker_known=marker_list_known[index_reliability]\n",
    "    marker_unknown=marker_list_unknown[index_reliability]\n",
    "    #1. empirical percentile lifetime (large-scale ; triangle)\n",
    "    t_critical_empirical=[]\n",
    "    for L_to_predict in L_list_predict:\n",
    "        N_to_predict=L_to_predict\n",
    "        lifetime_data=[]\n",
    "        file_name='./net_percentile_lifetime/PercentileLife_N='+str(L_to_predict)+'.txt'\n",
    "        f=open(file_name,'r')\n",
    "        line=f.readline()\n",
    "        while(line!=''):\n",
    "            lifetime_data.append(float(line))\n",
    "            line=f.readline()\n",
    "        f.close()\n",
    "        lifetime_data=lifetime_data[:500]\n",
    "        lifetime_data.sort()\n",
    "        rank=int((1-Reliability)*len(lifetime_data))\n",
    "        t_critical_empirical.append(lifetime_data[rank])\n",
    "    plt.scatter(L_list_predict,t_critical_empirical,marker=marker_unknown,edgecolors=color, facecolors='none',label='R='+str(Reliability)+\"(unknown)\",linewidth=line_width_marker_set,s=size_marker)\n",
    "    \n",
    "    #2. empirical percentile lifetime (small-scale ), which is used in fitting scaling law\n",
    "    t_critical_empirical=[]\n",
    "    for L_to_predict in L_list_train:\n",
    "        N_to_predict=L_to_predict\n",
    "        lifetime_data=[]\n",
    "        file_name='./net_percentile_lifetime/PercentileLife_N='+str(L_to_predict)+'.txt'\n",
    "        f=open(file_name,'r')\n",
    "        line=f.readline()\n",
    "        while(line!=''):\n",
    "            lifetime_data.append(float(line))\n",
    "            line=f.readline()\n",
    "        f.close()\n",
    "        lifetime_data=lifetime_data[:500]\n",
    "        lifetime_data.sort()\n",
    "        rank=int((1-Reliability)*len(lifetime_data))\n",
    "        t_critical_empirical.append(lifetime_data[rank])\n",
    "    plt.scatter(L_list_train,t_critical_empirical,marker=marker_known,edgecolors=color, facecolors='none',label='R='+str(Reliability)+\"(known)\",linewidth=line_width_marker_set,s=size_marker)\n",
    "\n",
    "    \n",
    "    \n",
    "    #3.prediction result (line)\n",
    "    N_test=np.logspace(start=2.7, stop=5.5, num=60)\n",
    "    rank=int((1-Reliability)*len(all_scaled_samples))\n",
    "    kappa=all_scaled_samples[rank]\n",
    "    t_critical_predict=target_function(np.array(N_test,dtype=float),best_mean_limit,c1)+kappa*N_test**(alpha)\n",
    "    plt.plot(N_test,t_critical_predict,linewidth=1,linestyle='--',label='R='+str(Reliability)+\"(predict)\")\n",
    "    #plt.scatter(x, y, s=100, edgecolors='b', facecolors='none', marker='o')\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"System Size \"+r\"$N$\",fontproperties=label_font)\n",
    "plt.ylabel(\"Percentile Lifetime / days\",fontproperties=label_font)\n",
    "plt.legend(prop=legend_font)\n",
    "ax = plt.gca()\n",
    "ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "plt.xticks([10**3,10**4,10**5])\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "\n",
    "plt.title(\"Percentile Lifetime\",fontproperties=label_font)\n",
    "filename='PercentileLifetime.png'\n",
    "plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16251a30",
   "metadata": {},
   "source": [
    "# Computation Time and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ad3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partitionGroundTruthSamples(groundTruthSamples,seperation=20):\n",
    "    interpoints=[] #\n",
    "    groundTruthSamples.sort()\n",
    "    for _ in range(1,seperation):\n",
    "        rank=int((_/seperation)*len(groundTruthSamples))\n",
    "        interpoints.append(groundTruthSamples[rank])\n",
    "    return interpoints\n",
    "#Test of this function:Partioning uniform distribution U[0,1] into ten intervals\n",
    "groundTruthSamples_test=list(np.random.random(1000000))\n",
    "interpoints=partitionGroundTruthSamples(groundTruthSamples_test,seperation=20)\n",
    "print(interpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulation time of sampling one sample for system with different size N\n",
    "# These data are recorded in my computer and used to estimate computation time below.\n",
    "time_cost_each_sample=dict()\n",
    "time_cost_each_sample[100]=0.32 #seconds\n",
    "time_cost_each_sample[200]=0.6 #seconds\n",
    "time_cost_each_sample[400]=1.7 #seconds\n",
    "time_cost_each_sample[800]=5.28 #seconds\n",
    "time_cost_each_sample[1600]=17.67 #seconds\n",
    "time_cost_each_sample[3200]=68.86 #seconds\n",
    "time_cost_each_sample[6400]=308 #seconds\n",
    "time_cost_each_sample[12800]=1320 #seconds\n",
    "time_cost_each_sample[25600]=4800 #seconds\n",
    "time_cost_each_sample[51200]=19380 #seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174226c9",
   "metadata": {},
   "source": [
    "Accuracy and Time cost of our method based on universal scaling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f03d890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "employ_sample_size_list=[10,14,20,28,40,55,70,90,110,140,170,200,250,300]\n",
    "\n",
    "\n",
    "N_list=100*2**np.array(range(2,10))\n",
    "L_list_train=N_list[:5]\n",
    "L_list_predict=N_list[5:]\n",
    "L_show_list=np.array(list(L_list_train)+list(L_list_predict))\n",
    "\n",
    "each_trian_sample_time=0\n",
    "for N in L_list_train:\n",
    "    each_trian_sample_time+=time_cost_each_sample[N]\n",
    "predict_error_dict=dict()\n",
    "total_train_time_list=each_trian_sample_time*np.array(employ_sample_size_list)\n",
    "KL_predict_dict=dict()\n",
    "for N_to_predict in L_list_predict:\n",
    "    KL_predict_dict[N_to_predict]=np.zeros(len(employ_sample_size_list))\n",
    "\n",
    "repeat_num= 30 # Repeat sampling and average to make result stable\n",
    "\n",
    "for index_employ_sample_size in range(len(employ_sample_size_list)):\n",
    "    employ_sample_size=employ_sample_size_list[index_employ_sample_size]\n",
    "    #Read NetLife\n",
    "    for _ in range(repeat_num):\n",
    "        std_all_list=[]\n",
    "        mean_all_list=[]\n",
    "        for N in N_list:\n",
    "            net_lifetime_list=[]\n",
    "            file_PercentileNetLife='./net_percentile_lifetime/PercentileLife_N='+str(N)+'.txt'\n",
    "            f=open(file_PercentileNetLife,'r')\n",
    "            line=f.readline()\n",
    "            while(line!=''):\n",
    "                net_lifetime_list.append(int(line))\n",
    "                line=f.readline()\n",
    "            f.close()\n",
    "            if(N in L_list_train):\n",
    "                net_lifetime_list=random.sample(net_lifetime_list,k=employ_sample_size)\n",
    "            else:\n",
    "                net_lifetime_list=net_lifetime_list\n",
    "            mean_all_list.append(np.mean(net_lifetime_list))\n",
    "            std_all_list.append(np.std(net_lifetime_list))\n",
    "        line_width_marker_set=2\n",
    "        dpi_set=1000\n",
    "        max_tick_num=4\n",
    "        mean_list=mean_all_list[:5]\n",
    "        std_list=std_all_list[:5]\n",
    "        mean_predict_list=mean_all_list[5:]\n",
    "        std_predict_list=std_all_list[5:]\n",
    "\n",
    "        x_to_train=np.array(L_list_train,dtype='float')\n",
    "        y_to_train=mean_list\n",
    "        x_to_predict=np.array(L_list_predict,dtype='float')\n",
    "        y_to_predict=np.array(mean_predict_list)\n",
    "\n",
    "        delta=1 #——————————————————————————————parameter：delta, the rate of convergence\n",
    "        power_for_weight_of_size=2  #——————————————————the weight of loss function \n",
    "        renormalized_x_to_train=x_to_train/np.sum(x_to_train)\n",
    "\n",
    "        def target_function(x,y0,c1):\n",
    "            return y0+c1*x**(-delta)\n",
    "        def loss_SQ(parms):\n",
    "            y0=parms[0]\n",
    "            c1=parms[1]\n",
    "            y_predict_list=y0+c1*x_to_train**(-delta)\n",
    "            res=y_predict_list-y_to_train\n",
    "            res=res*renormalized_x_to_train**power_for_weight_of_size/np.sum(renormalized_x_to_train**power_for_weight_of_size)\n",
    "            return np.sum(res**2)\n",
    "\n",
    "        initial_guess=[0,1]\n",
    "        result = minimize(loss_SQ, initial_guess, method='BFGS')\n",
    "\n",
    "        params=result.x\n",
    "        best_mean_limit,c1=params #detremining t_inf and c1.\n",
    "\n",
    "        plt.scatter(L_list_train,np.abs(best_mean_limit-mean_list),label='Known',edgecolors='#1f77b4', facecolors='none',marker='o',linewidth=line_width_marker_set,s=size_marker)\n",
    "        plt.scatter(L_list_predict,np.abs(best_mean_limit-mean_predict_list),label='Unknown',edgecolors='#1f77b4', facecolors='none',marker='^',linewidth=line_width_marker_set,s=size_marker)\n",
    "        x_to_show=np.linspace(np.min(x_to_train),np.max(x_to_predict),1000)\n",
    "        y_predict=target_function(x_to_show,best_mean_limit,c1)\n",
    "        plt.plot(x_to_show, np.abs(y_predict-best_mean_limit), color='red', label='Fit',linestyle='--')\n",
    "        r_value=np.corrcoef(x_to_show,np.abs(y_predict-best_mean_limit))[0][1]\n",
    "        print(\"R2=\"+str(round(r_value**2,3)))\n",
    "\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "        #ticks=tranform_ticks_log(np.min(value_list),np.max(value_list),max_tick_num)\n",
    "        #ticks=[0.2,0.3,0.4,0.6]\n",
    "        #plt.yticks(ticks)  \n",
    "\n",
    "\n",
    "        plt.xlabel(\"System Size \"+r\"$N$\",fontproperties=label_font)\n",
    "        plt.ylabel(r\"$<t_{\\infty}>-<t_N>$\",fontproperties=label_font)\n",
    "        #plt.title(net_name_show+\" (\"+\"power=\"+str(power)+\")\",fontproperties=label_font)\n",
    "        plt.legend(prop=legend_font)\n",
    "        plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "        plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "        filename='Mean_loglog.png'\n",
    "        plt.show()\n",
    "        \n",
    "        x_min_to_show=((np.min(mean_list)-best_mean_limit)/c1)**(1/(-delta))\n",
    "        x_show_list=np.linspace(x_min_to_show,L_list_predict[-1],100)\n",
    "\n",
    "        plt.scatter(L_list_train,mean_list,label='Known',edgecolors='#1f77b4', facecolors='none',marker='o',linewidth=line_width_marker_set,s=size_marker)\n",
    "        plt.scatter(L_list_predict,mean_predict_list,label='Unknown',edgecolors='#1f77b4', facecolors='none',marker='^',linewidth=line_width_marker_set,s=size_marker)\n",
    "\n",
    "        plt.plot(x_show_list, target_function(x_show_list,best_mean_limit,c1), color='red', label='Fit',linestyle='--')\n",
    "        print(r'$<t_{\\infty}>-$'+str(round(np.exp(intercept),4))+r'$N^{{{}}}$'.format(round(slope,4)))\n",
    "        plt.xlabel(\"System Size \"+r\"$N$\",fontproperties=label_font)\n",
    "        plt.ylabel(r\"$<t_N>$\",fontproperties=label_font)\n",
    "        #plt.title(net_name_show+\" (\"+\"power=\"+str(power)+\")\",fontproperties=label_font)\n",
    "        plt.legend(prop=legend_font)\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "        plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "        plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "        filename='Mean_predict.png'\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter(L_list_train,std_list,label='Known',edgecolors='#1f77b4', facecolors='none',marker='o',linewidth=line_width_marker_set,s=size_marker)\n",
    "        plt.scatter(L_list_predict,std_predict_list,label='Unknown',edgecolors='#1f77b4', facecolors='none',marker='^',linewidth=line_width_marker_set,s=size_marker)\n",
    "\n",
    "        x_list=np.log(L_list_train)\n",
    "        y_list=np.log(std_list)\n",
    "        slope=-1/2\n",
    "        intercept=np.mean(y_list)-slope*np.mean(x_list)\n",
    "\n",
    "        x_show_list=np.log(np.array(L_show_list))\n",
    "        y_show_list=slope * x_show_list + intercept\n",
    "\n",
    "        alpha=copy.deepcopy(slope) #——————————————————————————————————parameter： the scaling exponent for standard deviation. We set to 1/2 here.\n",
    "        regression_line = slope * x_list + intercept\n",
    "        #plt.plot(np.exp(x_list), np.exp(regression_line), color='red', label=str(round(np.exp(intercept),4))+r'$N^{{{}}}$'.format(round(slope,4)))\n",
    "        plt.plot(np.exp(x_show_list), np.exp(y_show_list), color='red', label=\"Fit\",linestyle='--')\n",
    "        print(str(round(np.exp(intercept),4))+r'$N^{{{}}}$'.format(round(slope,4)))\n",
    "\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "\n",
    "        #value_list=np.abs(best_mean_limit-mean_list)\n",
    "        #ticks=tranform_ticks_log(np.min(value_list),np.max(value_list),max_tick_num)\n",
    "        #plt.yticks(yticks)  \n",
    "\n",
    "        plt.xlabel(\"System Size \"+r\"$N$\",fontproperties=label_font)\n",
    "        plt.ylabel(r\"Standard deviation $\\chi_t(N)$\",fontproperties=label_font)\n",
    "        plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "        plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "        #plt.title(net_name_show+\" (\"+\"power=\"+str(power)+\")\",fontproperties=label_font)\n",
    "        plt.legend(prop=legend_font)\n",
    "        filename='std_predict.png'\n",
    "        plt.show()\n",
    "\n",
    "        marker_list=['^','v','>','<','o','d','*','1','2','3','4','+','x','|','_']\n",
    "        count_L=0\n",
    "        for L in np.array(list(L_list_train)):\n",
    "            marker=marker_list[count_L]\n",
    "            count_L+=1\n",
    "            N=L\n",
    "            lifetime_data=[]\n",
    "            file_name='./net_percentile_lifetime/PercentileLife_N='+str(L)+'.txt'\n",
    "            f=open(file_name,'r')\n",
    "            line=f.readline()\n",
    "            while(line!=''):\n",
    "                lifetime_data.append(float(line))\n",
    "                line=f.readline()\n",
    "            f.close()\n",
    "            lifetime_data=lifetime_data[:500]\n",
    "            min_life=np.min(lifetime_data)\n",
    "            max_life=np.max(lifetime_data)\n",
    "            cut_list=np.linspace(min_life,max_life,20)\n",
    "            mid_list=[]\n",
    "            distri_value_list=[]\n",
    "            for index_cut in range(len(cut_list)-1):\n",
    "                down=cut_list[index_cut]\n",
    "                up=cut_list[index_cut+1]\n",
    "                mid_list.append(((up+down)/2))\n",
    "                distri_value_list.append(np.sum((lifetime_data>down)&(lifetime_data<up))/(len(lifetime_data)*(up-down)))\n",
    "            distri_value_list=np.array(distri_value_list)\n",
    "            mid_list=np.array(mid_list)\n",
    "            plt.scatter((mid_list-np.mean(lifetime_data))/N**alpha,distri_value_list*N**alpha,label='N='+str(N),marker=marker,s=size_marker)\n",
    "            plt.plot((mid_list-np.mean(lifetime_data))/N**alpha,distri_value_list*N**alpha,linewidth=1)\n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "        #plt.xticks([-50,0,50])\n",
    "        #plt.xlim(-100,150)\n",
    "        plt.legend(loc='upper right',prop=legend_small_font)\n",
    "        plt.xlabel(\"Rescaled Deviation \"+r\"$(t_N-<t_N>)$\"+r'$N^{{{}}}$'.format(round(-alpha,4)),fontproperties=label_font)\n",
    "        plt.ylabel(\"Rescaled Probability \"+r\"$P(t)/$\"+r'$N^{{{}}}$'.format(round(-alpha,4)),fontproperties=label_font)\n",
    "        plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "        plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size) \n",
    "        filename='universalDistribution.png'\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print('c1='+str(round(c1,4))+' delta='+str(round(delta,4))+' alpha='+str(round(-alpha,4)))\n",
    "        rescaled_lifetime_dict=dict()\n",
    "        all_scaled_samples=[]\n",
    "        for L in L_list_train:\n",
    "            N=L\n",
    "            lifetime_data=[]\n",
    "            file_name='./net_percentile_lifetime/PercentileLife_N='+str(L)+'.txt'\n",
    "            f=open(file_name,'r')\n",
    "\n",
    "            line=f.readline()\n",
    "            while(line!=''):\n",
    "                lifetime_data.append(float(line))\n",
    "                line=f.readline()\n",
    "            f.close()\n",
    "            lifetime_data=lifetime_data[:employ_sample_size]\n",
    "            min_life=np.min(lifetime_data)\n",
    "            max_life=np.max(lifetime_data)\n",
    "            cut_list=np.linspace(min_life,max_life,20)\n",
    "            mid_list=[]\n",
    "            distri_value_list=[]\n",
    "            for index_cut in range(len(cut_list)-1):\n",
    "                down=cut_list[index_cut]\n",
    "                up=cut_list[index_cut+1]\n",
    "                mid_list.append(((up+down)/2))\n",
    "                distri_value_list.append(np.sum((lifetime_data>down)&(lifetime_data<up))/(len(lifetime_data)*(up-down)))\n",
    "            rescaled_lifetime_data=(lifetime_data-np.mean(lifetime_data))/N**(alpha)\n",
    "            rescaled_lifetime_data.sort()\n",
    "            rescaled_lifetime_dict[N]=rescaled_lifetime_data\n",
    "            all_scaled_samples=all_scaled_samples+list(rescaled_lifetime_data)\n",
    "            acc_list=np.array(list(range(1,len(rescaled_lifetime_data)+1)))/len(rescaled_lifetime_data)\n",
    "            plt.plot(rescaled_lifetime_data,acc_list,label='N='+str(N))\n",
    "        plt.legend(prop=legend_font)\n",
    "\n",
    "        ax = plt.gca()\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "        ax.xaxis.set_major_locator(MaxNLocator(nbins=4))\n",
    "\n",
    "        plt.ylabel(\"Cumulative probability\",fontproperties=label_font)\n",
    "        plt.xlabel(\"Rescaled Deviation \"+r\"$(t_N-<t_N>)$\"+r'$N^{{{}}}$'.format(round(-alpha,4)),fontproperties=label_font)\n",
    "        plt.close()\n",
    "\n",
    "        for L_to_predict in L_list_predict:\n",
    "            N_to_predict=L_to_predict\n",
    "            predict_distributions=target_function(float(N_to_predict),best_mean_limit,c1)+np.array(all_scaled_samples)*N_to_predict**(alpha)\n",
    "            lifetime_data=[]\n",
    "            file_name='./net_percentile_lifetime/PercentileLife_N='+str(N_to_predict)+'.txt'\n",
    "            f=open(file_name,'r')\n",
    "            line=f.readline()\n",
    "            while(line!=''):\n",
    "                lifetime_data.append(float(line))\n",
    "                line=f.readline()\n",
    "            f.close()\n",
    "            lifetime_data=lifetime_data[:500]\n",
    "            lifetime_data.sort()\n",
    "\n",
    "            plt.hist(lifetime_data,alpha=0.5,density=True,bins=20,label='GroundTruth')\n",
    "            plt.hist(predict_distributions,alpha=0.5,density=True,bins=20,label='Predict')\n",
    "            plt.legend()\n",
    "            plt.close()\n",
    "\n",
    "            groundTruthSamples=lifetime_data\n",
    "            interpoints=partitionGroundTruthSamples(groundTruthSamples,seperation=10)\n",
    "            KL_predict=KL_div_calGivenInterpoints(interpoints,predict_distributions,groundTruthSamples)\n",
    "            print(\"KL=\"+str(KL_predict))\n",
    "            KL_predict_dict[N_to_predict][index_employ_sample_size]+=KL_predict/repeat_num\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da1b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#draw figure for relationship between error and computation time for USF method\n",
    "for N in L_list_predict:\n",
    "    plt.scatter(KL_predict_dict[N],total_train_time_list/3600,label='USF Method(N='+str(N)+\")\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Error (KL divergence with Ground Truth)\")\n",
    "plt.ylabel(\"Computation Time / hours\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46edfc",
   "metadata": {},
   "source": [
    "Accuracy and Time cost of direct simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbe744",
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_samples_num=[5,10,14,20,28,40,56,79,100]\n",
    "time_direct_dict=dict()\n",
    "KL_direct_dict=dict()\n",
    "repeat_num= 30 # Repeat sampling to make result stable\n",
    "\n",
    "for N_to_predict in L_list_predict:\n",
    "    KL_direct_dict[N_to_predict]=[]\n",
    "for N in L_list_predict:\n",
    "    time_direct_dict[N]=time_cost_each_sample[N]*np.array(direct_samples_num)\n",
    "for L_to_predict in L_list_predict:\n",
    "    \n",
    "    N_to_predict=L_to_predict\n",
    "    lifetime_data=[]\n",
    "    file_name='./net_percentile_lifetime/PercentileLife_N='+str(N_to_predict)+'.txt'\n",
    "    f=open(file_name,'r')\n",
    "    line=f.readline()\n",
    "    while(line!=''):\n",
    "        lifetime_data.append(float(line))\n",
    "        line=f.readline()\n",
    "    f.close()\n",
    "    lifetime_data=lifetime_data[:500]\n",
    "    \n",
    "    for direct_sample_num in direct_samples_num:\n",
    "        KL_direct_ave=0\n",
    "        for _ in range(repeat_num):\n",
    "            direct_data=random.sample(lifetime_data,direct_sample_num)\n",
    "            groundTruthSamples=lifetime_data\n",
    "            interpoints=partitionGroundTruthSamples(groundTruthSamples,seperation=10)\n",
    "            KL_direct=KL_div_calGivenInterpoints(interpoints,direct_data,groundTruthSamples)\n",
    "            KL_direct_ave+=KL_direct\n",
    "        KL_direct_ave/=repeat_num\n",
    "        print(\"KL=\"+str(KL_direct))\n",
    "        KL_direct_dict[N_to_predict].append(KL_direct_ave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476faab5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for N in L_list_predict:\n",
    "    plt.scatter(KL_direct_dict[N],time_direct_dict[N]/3600,label='Direct(N='+str(N)+\")\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Error (KL divergence with Ground Truth)\")\n",
    "plt.ylabel(\"Computation Time / hours\")\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f17cfe",
   "metadata": {},
   "source": [
    "The result of direct simulation and USF method in one figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fc2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparision between USF method and Direct simulation\n",
    "\n",
    "\n",
    "default_colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] #color setting\n",
    "default_colors=['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "for index_N in range(len(L_list_predict)):\n",
    "    N=L_list_predict[index_N]\n",
    "    color_to_show=default_colors[index_N]\n",
    "    plt.scatter(KL_direct_dict[N],time_direct_dict[N]/3600,label='Direct(N='+str(N)+\")\",marker='o',edgecolors=color_to_show, facecolors='none',linewidth=line_width_marker_set,s=size_marker)\n",
    "    plt.scatter(KL_predict_dict[N],total_train_time_list/3600,label='USF(N='+str(N)+\")\",marker='^',edgecolors=color_to_show, facecolors='none',linewidth=line_width_marker_set,s=size_marker)\n",
    "plt.subplots_adjust(right=1.0)\n",
    "plt.legend(prop=legend_font)\n",
    "plt.xlabel(\"Error (KL divergence)\",fontproperties=label_font)\n",
    "plt.ylabel(\"Computation Time / hours\",fontproperties=label_font)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size) \n",
    "filename='ComputationTimeAndAccuracy_legendIn.png'\n",
    "plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparision between USF method and Direct simulation\n",
    "\n",
    "\n",
    "default_colors = plt.rcParams['axes.prop_cycle'].by_key()['color'] #color setting\n",
    "default_colors=['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "for index_N in range(len(L_list_predict)):\n",
    "    N=L_list_predict[index_N]\n",
    "    color_to_show=default_colors[index_N]\n",
    "    plt.scatter(KL_direct_dict[N],time_direct_dict[N]/3600,label='Direct(N='+str(N)+\")\",marker='o',edgecolors=color_to_show, facecolors='none',linewidth=line_width_marker_set,s=size_marker)\n",
    "    plt.scatter(KL_predict_dict[N],total_train_time_list/3600,label='USF(N='+str(N)+\")\",marker='^',edgecolors=color_to_show, facecolors='none',linewidth=line_width_marker_set,s=size_marker)\n",
    "plt.subplots_adjust(right=1.0)\n",
    "plt.legend(prop=legend_font,loc=8,bbox_to_anchor=(1.2,0.5))\n",
    "plt.xlabel(\"Error (KL divergence)\",fontproperties=label_font)\n",
    "plt.ylabel(\"Computation Time / hours\",fontproperties=label_font)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xticks(fontproperties = 'Times New Roman', size = special_tick_size)\n",
    "plt.yticks(fontproperties = 'Times New Roman', size = special_tick_size) \n",
    "filename='ComputationTimeAndAccuracy_legendOut.png'\n",
    "plt.savefig(\"./fig_case/\"+filename,dpi=dpi_set,bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14d57b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
