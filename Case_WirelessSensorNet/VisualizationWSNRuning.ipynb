{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da20e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import matplotlib.font_manager as font_manager\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.patches import Polygon,Circle\n",
    "\n",
    "\n",
    "title_font = font_manager.FontProperties(family='Times New Roman', size=16, weight='normal')\n",
    "label_font = font_manager.FontProperties(family='Times New Roman', size=16, weight='bold')\n",
    "legend_font = font_manager.FontProperties(family='Times New Roman', size=14, weight='normal')\n",
    "special_tick_size=15\n",
    "size_marker=90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a12dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_line_to_axes(point1, point2, xlim, ylim):\n",
    "    # If the x-coordinates of the two points are the same, the line is vertical to the x-axis\n",
    "    if point1[0] == point2[0]:\n",
    "        x = point1[0]\n",
    "        y_bottom = ylim[0]\n",
    "        y_top = ylim[1]\n",
    "    else:\n",
    "        # Calculate the slope and intercept of the line connecting the two points\n",
    "        slope = (point2[1] - point1[1]) / (point2[0] - point1[0])\n",
    "        intercept = point1[1] - slope * point1[0]\n",
    "\n",
    "        # Calculate the two endpoints of the dashed line based on xlim\n",
    "        x_left = xlim[0]\n",
    "        y_left = slope * x_left + intercept\n",
    "\n",
    "        x_right = xlim[1]\n",
    "        y_right = slope * x_right + intercept\n",
    "\n",
    "        # If the dashed line intersects the lower bound of ylim, take the point on the lower bound\n",
    "        y_bottom = ylim[0] if y_left < ylim[0] else y_left\n",
    "\n",
    "        # If the dashed line intersects the upper bound of ylim, take the point on the upper bound\n",
    "        y_top = ylim[1] if y_right > ylim[1] else y_right\n",
    "\n",
    "    return x_left, y_left, x_right, y_right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4a74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Energy Comsuption Calculation\n",
    "\n",
    "E_elect=50*10**(-9) # 50 nJ/ bit\n",
    "e_amp=100*10**(-12) #100 pJ/bit/m^2 the transmit amplifier \n",
    "bit_each_packet=2000 #2000 bit for each meassage\n",
    "e_computation=5*10**(-9) # 5 nJ/ bit/ message\n",
    "initial_capacity=0.5 # initial energy of sensor nodes\n",
    "period_sendingMessage=1 # Sending each message during 1 day\n",
    "\n",
    "#Calculate Energy Comsuption for Sending Message\n",
    "def EnergySend(k,distance):\n",
    "    return E_elect*k+e_amp*k*distance**2\n",
    "\n",
    "#Calculate Energy Comsuption for Recieving Message\n",
    "def EnergyRecieve(k):\n",
    "    return E_elect*k\n",
    "\n",
    "#Calculate Energy Comsuption for Fusing Message\n",
    "def EnergyComputation(k):\n",
    "    return e_computation*k\n",
    "#Module 2:Calculate failure Rate\n",
    "\n",
    "failure_rate_compare=10**(-3) #10^(-3) day^(-1)\n",
    "def calFailureRate(capacity):\n",
    "    return failure_rate_compare\n",
    "def calTimeToFailure(failure_rate,period_sendingMessage):\n",
    "    time_to_failure=-np.log(1-random.random())/failure_rate\n",
    "    return time_to_failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clusterHeads(living_nodes,round_last_head,P,round_count):\n",
    "    clusterHead_list=[]\n",
    "    for node in living_nodes:\n",
    "        if(round_last_head[node]<(round_count-int(1/P))): #if this node have not been cluster-head in the last 1/P rounds\n",
    "            threshold=P/(1-P*(round_count%int(1/P)))\n",
    "            if(random.random()<threshold):\n",
    "                clusterHead_list.append(node)\n",
    "    return clusterHead_list\n",
    "\n",
    "def distributeLivingNodesToHead(living_nodes,pos_list,clusterHead_list):\n",
    "    head_map=dict()\n",
    "    living_nodes_not_head=set(living_nodes)-set(clusterHead_list)\n",
    "    for not_head_node in living_nodes_not_head:\n",
    "        distance_square_to_Head_min=float('inf')\n",
    "        Head_min=-1\n",
    "        for head in clusterHead_list:\n",
    "            distance_square_to_Head=(pos_list[not_head_node][0]-pos_list[head][0])**2+(pos_list[not_head_node][1]-pos_list[head][1])**2\n",
    "            if(distance_square_to_Head<distance_square_to_Head_min):\n",
    "                Head_min=head\n",
    "                distance_square_to_Head_min=distance_square_to_Head\n",
    "        head_map[not_head_node]=(Head_min,distance_square_to_Head_min)\n",
    "    return head_map\n",
    "\n",
    "def calEnergyConsumptionEach(living_nodes,clusterHead_list,pos_list,head_map,basic_information=2000):\n",
    "    capacity_consumed=dict()\n",
    "    living_nodes_not_head=set(living_nodes)-set(clusterHead_list)\n",
    "    information_to_process_for_head=dict()\n",
    "    if(clusterHead_list==[]):\n",
    "        for node in living_nodes:\n",
    "            capacity_consumed[node]=0\n",
    "        return capacity_consumed\n",
    "    for head in clusterHead_list:\n",
    "        information_to_process_for_head[head]=0\n",
    "    for node_not_head in living_nodes_not_head:\n",
    "        Head_min,distance_square_to_Head_min=head_map[node_not_head]\n",
    "        information_upload=basic_information\n",
    "        information_to_process_for_head[Head_min]+=information_upload\n",
    "        capacity_consumed[node_not_head]=EnergySend(information_upload,(distance_square_to_Head_min)**(1/2))\n",
    "    for clusterHead in clusterHead_list:\n",
    "        information=information_to_process_for_head[clusterHead]\n",
    "        distance=np.sqrt(pos_list[clusterHead][0]**2+pos_list[clusterHead][1]**2)\n",
    "        basic_information=2000\n",
    "        capacity_consumed[clusterHead]=EnergyRecieve(information)+EnergyComputation(information)+EnergySend(basic_information,distance)\n",
    "    return capacity_consumed\n",
    "\n",
    "def CalRemainEnergyJudgeDeadAndUnsensoredNode(capacity_consumed,capacity_list,clusterHead_list,living_nodes,head_map):\n",
    "    newly_dead_nodes=[] #dead nodes due to wear-out of battery\n",
    "    unsensor_living_nodes=[] #Living nodes that can not connect with Base Station due to failure of Cluster-Head\n",
    "    living_nodes_not_head=set(living_nodes)-set(clusterHead_list)\n",
    "    for node in living_nodes:\n",
    "        capacity_list[node]-=capacity_consumed[node]\n",
    "        if(capacity_list[node]<0):\n",
    "            newly_dead_nodes.append(node)\n",
    "    for living_node_not_head in (set(living_nodes_not_head)-set(newly_dead_nodes)):\n",
    "        Head_min,distance_square_to_Head_min=head_map[living_node_not_head]\n",
    "        if Head_min in newly_dead_nodes:\n",
    "            unsensor_living_nodes.append(living_node_not_head)\n",
    "    return newly_dead_nodes,unsensor_living_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed3563f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initialize the position of nodes\n",
    "N=400\n",
    "pos_list=[]\n",
    "file_name='./structure_WSN/networkUsedInPaper/randomSquare_N='+str(N)+'.txt'\n",
    "f=open(file_name,'r')\n",
    "line=f.readline()\n",
    "while(line!=''):\n",
    "    for _ in range(len(line)):\n",
    "        if(line[_]==' '):\n",
    "            s=_\n",
    "    x=float(line[:s])\n",
    "    y=float(line[s:])\n",
    "    pos_list.append([x,y])\n",
    "    line=f.readline()\n",
    "f.close()\n",
    "\n",
    "capacity_mean_living_history=[]\n",
    "living_num_history=[]\n",
    "percentiles_show_list=list(np.arange(1,0.78,-0.01)) # we demonstrate snapshot of system when fraction of living nodes is 100%,99%,98%,...,1%\n",
    "index_show_percentile=0 \n",
    "#Simulation in one runs\n",
    "capacity_list=np.ones(N)*initial_capacity #initialize capacity of all sensor nodes\n",
    "living_nodes=set(list(range(N)))\n",
    "lifetime_node=dict()\n",
    "\n",
    "days_in_a_round=10 # update cluster-head for each 5 days\n",
    "P=0.05 #fraction of cluster heads\n",
    "round_last_head=np.ones(N)*(-2*int(1/P)) #the last round when node is cluster-head\n",
    "round_count=0 #\n",
    "day_count=0 #\n",
    "radius_sensor=3\n",
    "\n",
    "while(len(list(living_nodes))>0):\n",
    "    clusterHead_list=select_clusterHeads(living_nodes,round_last_head,P,round_count)\n",
    "    for clusterHead in clusterHead_list:\n",
    "        round_last_head[clusterHead]=round_count\n",
    "    head_map=distributeLivingNodesToHead(living_nodes,pos_list,clusterHead_list)\n",
    "    \n",
    "    percentile_live=len(list(living_nodes))/N#If , demonstrate\n",
    "    if(abs(len(clusterHead_list)-0.05*len(list(living_nodes)))<100):\n",
    "        if(percentile_live<=percentiles_show_list[index_show_percentile]):\n",
    "            if(clusterHead_list==[]):\n",
    "                continue\n",
    "            percentile_show=str(percentiles_show_list[index_show_percentile])\n",
    "            living_nodes_not_head=set(living_nodes)-set(clusterHead_list)\n",
    "            set_DeadSensors=set(range(N))-set(living_nodes)\n",
    "            points_clusterHead=[]\n",
    "            points_livingSensorsNormal=[]\n",
    "            points_deadSensors=[]\n",
    "            for clusterHead in clusterHead_list:\n",
    "                points_clusterHead.append(pos_list[clusterHead])\n",
    "            for livingSensors in living_nodes_not_head:\n",
    "                points_livingSensorsNormal.append(pos_list[livingSensors])\n",
    "            for deadSensors in set_DeadSensors:\n",
    "                points_deadSensors.append(pos_list[deadSensors])\n",
    "            points_living=np.array(points_clusterHead+points_livingSensorsNormal)\n",
    "            points_clusterHead,points_deadSensors,points_livingSensorsNormal,=np.array(points_clusterHead),np.array(points_deadSensors),np.array(points_livingSensorsNormal)\n",
    "\n",
    "            ax = plt.gca()\n",
    "            plt.xlim([-25, 25])\n",
    "            plt.ylim([0, 50])\n",
    "\n",
    "            if(len(clusterHead_list)==2):\n",
    "                vec_two_point=[points_clusterHead[0][0]-points_clusterHead[1][0],points_clusterHead[0][1]-points_clusterHead[1][1]]\n",
    "                mid_point=[(points_clusterHead[0][0]+points_clusterHead[1][0])/2,(points_clusterHead[0][1]+points_clusterHead[1][1])/2]\n",
    "                vec_vertical=[vec_two_point[1],-vec_two_point[0]]\n",
    "                another_point=np.array(mid_point)+np.array(vec_vertical)\n",
    "                x_left, y_left, x_right, y_right = extend_line_to_axes(mid_point, another_point, [-25,25], [0,50])\n",
    "                # Plot the dashed line\n",
    "                plt.plot([x_left, x_right], [y_left, y_right], c='black', linestyle='--',linewidth=1)\n",
    "            if(len(clusterHead_list)>2):\n",
    "                vor = Voronoi(points_clusterHead)\n",
    "                voronoi_plot_2d(vor,show_points=False,show_vertices=False,show_input=False,ax=ax,linewidth=1)\n",
    "\n",
    "            plt.scatter(points_livingSensorsNormal[:, 0], points_livingSensorsNormal[:, 1],color='orange', marker='o',label='Living nodes',s=30)\n",
    "            if(len(points_deadSensors)!=0):\n",
    "                plt.scatter(points_deadSensors[:, 0], points_deadSensors[:, 1],color='red', marker='+',label='Failed nodes')\n",
    "            else:\n",
    "                plt.scatter(None,None,color='red', marker='+',label='Failed nodes')\n",
    "            plt.scatter(points_clusterHead[:, 0], points_clusterHead[:, 1],facecolor='None',edgecolor='green', marker='*',s=200,label='Cluster heads')\n",
    "\n",
    "            plt.scatter(0,0,color='black',marker='^',s=250)        \n",
    "            plt.scatter(None,None,color='black',marker='^',s=50,label='Base Station')\n",
    "\n",
    "            plt.xlim([-25, 25])\n",
    "            plt.ylim([0, 50])\n",
    "            plt.title(str(int(100*(1-len(list(living_nodes))/N)))+\"% nodes failed\",fontproperties=label_font)\n",
    "            plt.xlabel(\"x coordinate\",fontproperties=label_font)\n",
    "            plt.ylabel(\"y coordinate\",fontproperties=label_font)\n",
    "            plt.legend(loc='upper left')\n",
    "            #plt.savefig(\"./demo/N=\"+str(N)+\"_\"+str(int(100*(1-len(list(living_nodes))/N)))+\"%failed.png\",dpi=400,bbox_inches='tight')\n",
    "            plt.show()\n",
    "            index_show_percentile+=1\n",
    "    if(index_show_percentile==len(percentiles_show_list)-1):\n",
    "        break\n",
    "    for day in range(days_in_a_round):\n",
    "        capacity_consumed=calEnergyConsumptionEach(living_nodes,clusterHead_list,pos_list,head_map,basic_information=2000)\n",
    "        newly_dead_nodes,unsensor_living_nodes=CalRemainEnergyJudgeDeadAndUnsensoredNode(capacity_consumed,capacity_list,clusterHead_list,living_nodes,head_map)\n",
    "        for node in newly_dead_nodes:\n",
    "            lifetime_node[node]=day_count\n",
    "        living_nodes=living_nodes-set(newly_dead_nodes) # nodes die of battery wear-out\n",
    "        day_count+=1\n",
    "    node_to_failure=[]\n",
    "    \n",
    "    capacity_sum=0\n",
    "    living_num=len(list(living_nodes))\n",
    "    for node in living_nodes:\n",
    "        capacity=capacity_list[node]\n",
    "        capacity_sum+=capacity\n",
    "        failure_rate=calFailureRate(capacity)\n",
    "        period_sendingMessage=1\n",
    "        failure_prob=1-np.exp(-failure_rate*period_sendingMessage)\n",
    "        if(random.random()<failure_prob):\n",
    "            node_to_failure.append(node)\n",
    "            lifetime_node[node]=day_count\n",
    "    living_num_history.append(living_num)\n",
    "    capacity_mean_living_history.append(capacity_sum/len(living_nodes))\n",
    "    living_nodes=living_nodes-set(node_to_failure) #nodes die of failure\n",
    "    round_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae107e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
